{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.utils.data as utils\n",
    "from matplotlib import image as img\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, y, optimizer, criterion, batch_size, lr, p=30):\n",
    "    X = torch.Tensor(X).float()\n",
    "    N = len(X)\n",
    "    y = torch.Tensor(y).long()\n",
    "    train_size = int(0.75 * N)\n",
    "    valid_size = N - train_size\n",
    "    train_index, valid_index = torch.utils.data.random_split(\n",
    "        np.arange(N), [train_size, valid_size]\n",
    "    )\n",
    "    train_dataset = utils.TensorDataset(X[train_index], y[train_index])\n",
    "    valid_dataset = utils.TensorDataset(X[valid_index], y[valid_index])\n",
    "    dataloader = utils.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "    validloader = utils.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "    j = 0\n",
    "    epoch = 0\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_vloss = float(\"inf\")\n",
    "    while j < p:\n",
    "        epoch += 1\n",
    "        net.train_mode()\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            X, y = batch\n",
    "            X = X.view(-1, 3, 64, 64)\n",
    "            y = y.view(-1)\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            loss = criterion(net.forward(X), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        net.eval_mode()\n",
    "        train_loss, train_acc = net.evaluate(dataloader, criterion)\n",
    "        valid_loss, valid_acc = net.evaluate(validloader, criterion)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        if valid_loss < best_vloss:\n",
    "            best_vacc = valid_acc\n",
    "            best_vloss = valid_loss\n",
    "            best_net = net\n",
    "            best_epoch = epoch\n",
    "            j = 0\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "        print(\" [LOSS] TRAIN {} / VALID {}\".format(train_loss, valid_loss))\n",
    "        print(\" [ACC] TRAIN {} / VALID {}\".format(train_acc, valid_acc))\n",
    "    best_model = {\n",
    "        \"net\": best_net.state_dict(),\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"final_epoch\": epoch,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"vacc\": best_vacc,\n",
    "        \"vloss\": valid_losses,\n",
    "        \"taccs\": train_accs,\n",
    "        \"tloss\": train_losses,\n",
    "        \"vaccs\": valid_accs,\n",
    "        \"convs\": net.conv_size,\n",
    "        \"lins\": net.lin_size,\n",
    "    }\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def accuracy(y_pred, target):\n",
    "    correct = torch.eq(y_pred.max(1)[1], target).sum().type(torch.FloatTensor)\n",
    "    return correct / len(target)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p=0.3):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = x.data\n",
    "        shape = data.shape\n",
    "        size = int(shape[0] * shape[1])\n",
    "        drop_idx = np.random.choice(\n",
    "            np.arange(size), replace=False, size=int(size * self.p)\n",
    "        )\n",
    "        data = data.flatten()\n",
    "        data[drop_idx] = 0\n",
    "        data = data.reshape(shape)\n",
    "        x.data = data\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, conv_size, lin_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.drop = True\n",
    "        self.lin_size = lin_size\n",
    "        self.conv_size = conv_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_size, conv_size, 3, 1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(conv_size, conv_size * 2, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(conv_size * 2, conv_size * 2, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(3, 3),\n",
    "            nn.ReLU(True),\n",
    "            Flatten(),\n",
    "            nn.Linear(conv_size * 162, lin_size)).cuda()\n",
    "        self.dropout = nn.Sequential(Dropout()).cuda()\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(lin_size, 2),\n",
    "        ).cuda()\n",
    "\n",
    "    def eval_mode(self):\n",
    "        self.drop = False\n",
    "        \n",
    "    def train_mode(self):\n",
    "        self.drop = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dat = self.model(x)\n",
    "        if self.drop:\n",
    "            dat = self.dropout(dat)\n",
    "        return self.lin(dat)\n",
    "\n",
    "    def evaluate(self, dataloader, criterion):\n",
    "        self.eval_mode()\n",
    "        LOSSES = 0\n",
    "        ACCURACY = 0\n",
    "        COUNTER = 0\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X.view(-1, 3, 64, 64)\n",
    "            y = y.view(-1)\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            loss = criterion(self.forward(X), y)\n",
    "            acc = accuracy(self.forward(X), y)\n",
    "            n = y.size(0)\n",
    "            LOSSES += loss.sum().data.cpu().numpy() * n\n",
    "            ACCURACY += acc.sum().data.cpu().numpy() * n\n",
    "            COUNTER += n\n",
    "\n",
    "        floss = LOSSES / float(COUNTER)\n",
    "        faccuracy = ACCURACY / float(COUNTER)\n",
    "        return floss, faccuracy\n",
    "    \n",
    "    def train(self, X, y, epochs, optimizer, criterion, batch_size, lr):\n",
    "        train_dataset = utils.TensorDataset(X, y)\n",
    "        dataloader = utils.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "        )\n",
    "        train_accs = []\n",
    "        train_losses = []\n",
    "        for e in epochs:\n",
    "            net.train_mode()\n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                X, y = batch\n",
    "                X = X.view(-1, 3, 64, 64)\n",
    "                y = y.view(-1)\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "                loss = criterion(net.forward(X), y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            net.eval_mode()\n",
    "            train_loss, train_acc = net.evaluate(dataloader, criterion)\n",
    "\n",
    "            train_accs.append(train_acc)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            print(\"epoch: {}\".format(epoch))\n",
    "            print(\"TRAIN : [LOSS] {} / [ACC] {}\".format(train_loss, train_acc))\n",
    "        print('Training Done')\n",
    "\n",
    "\n",
    "def create_dataset(cats_path, dogs_path, test_path):\n",
    "    train_set = []\n",
    "    target = []\n",
    "    n_cats = len(os.listdir(cats_path))\n",
    "    for i in range(1, n_cats):\n",
    "        f = '{}.Cat.jpg'.format(i)\n",
    "        mat = img.imread(cats_path + f)\n",
    "        if not len(mat.shape) == 3:\n",
    "            mat = np.array((mat, mat, mat)).T\n",
    "        train_set.append(mat)\n",
    "        target.append(0)\n",
    "\n",
    "    n_dogs = len(os.listdir(dogs_path))\n",
    "    for i in range(1, n_dogs):\n",
    "        f = '{}.Dog.jpg'.format(i)\n",
    "        mat = img.imread(dogs_path + f)\n",
    "        if not len(mat.shape) == 3:\n",
    "            mat = np.array((mat, mat, mat)).T\n",
    "        train_set.append(mat)\n",
    "        target.append(1)\n",
    "\n",
    "    train_set = np.asarray(train_set)\n",
    "    target = np.asarray(target)\n",
    "    \n",
    "    test_set = []\n",
    "    n_test = len(os.listdir(test_path))\n",
    "    for i in range(1, n_test):\n",
    "        f = '{}.jpg'.format(i)\n",
    "        mat = img.imread(test_path + f)\n",
    "        if not len(mat.shape) == 3:\n",
    "            mat = np.array((mat, mat, mat)).T\n",
    "        test_set.append(mat)\n",
    "\n",
    "    return train_set, target, np.asarray(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_path = \"/home/arthur/git/IFT6135_assignment_1/testset/test/\"\n",
    "dogs_path = \"/home/arthur/git/IFT6135_assignment_1/trainset/Dog/\"\n",
    "cats_path = \"/home/arthur/git/IFT6135_assignment_1/trainset/Cat/\"\n",
    "X, y, submission = create_dataset(cats_path, dogs_path, test_path)\n",
    "X = X / 255.0\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network contains 2712066 parameters.\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 32, 512)\n",
    "par = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The network contains {} parameters.\".format(par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      " [LOSS] TRAIN 0.6700874076824693 / VALID 0.6672149954926707\n",
      " [ACC] TRAIN 0.5871174234866842 / VALID 0.5951190238107226\n",
      "epoch: 2\n",
      " [LOSS] TRAIN 0.6690453996799497 / VALID 0.6654292069856919\n",
      " [ACC] TRAIN 0.5909181836645484 / VALID 0.5985197039646348\n",
      "epoch: 3\n",
      " [LOSS] TRAIN 0.6677214782532274 / VALID 0.6640255659741148\n",
      " [ACC] TRAIN 0.5938521037858796 / VALID 0.5991198239886396\n",
      "epoch: 4\n",
      " [LOSS] TRAIN 0.6751243627142126 / VALID 0.6705143414132236\n",
      " [ACC] TRAIN 0.5832499833578204 / VALID 0.5887177435546714\n",
      "epoch: 5\n",
      " [LOSS] TRAIN 0.6826956946507863 / VALID 0.6805772873896984\n",
      " [ACC] TRAIN 0.5785157031505642 / VALID 0.5789157831924012\n",
      "epoch: 6\n",
      " [LOSS] TRAIN 0.6929893692731937 / VALID 0.6902996927982665\n",
      " [ACC] TRAIN 0.5737147429843596 / VALID 0.5747149430124443\n",
      "epoch: 7\n",
      " [LOSS] TRAIN 0.6637925642915207 / VALID 0.6591035415110862\n",
      " [ACC] TRAIN 0.6040541441899868 / VALID 0.6079215843228251\n",
      "epoch: 8\n",
      " [LOSS] TRAIN 0.6728129170612946 / VALID 0.6688992260312719\n",
      " [ACC] TRAIN 0.592451823725894 / VALID 0.593918783762713\n",
      "epoch: 9\n",
      " [LOSS] TRAIN 0.6709201224980421 / VALID 0.6679247444118112\n",
      " [ACC] TRAIN 0.593251983753926 / VALID 0.5955191038267258\n",
      "epoch: 10\n",
      " [LOSS] TRAIN 0.6651956381945322 / VALID 0.6608687886108658\n",
      " [ACC] TRAIN 0.6025205041405646 / VALID 0.6073214642928586\n",
      "epoch: 11\n",
      " [LOSS] TRAIN 0.6608235895605876 / VALID 0.6580164116915523\n",
      " [ACC] TRAIN 0.6063212642588123 / VALID 0.6135227045468699\n",
      "epoch: 12\n",
      " [LOSS] TRAIN 0.6603610083056409 / VALID 0.6567498351316686\n",
      " [ACC] TRAIN 0.6091885044033101 / VALID 0.6129225845526733\n",
      "epoch: 13\n",
      " [LOSS] TRAIN 0.6598595613724885 / VALID 0.6562311495392531\n",
      " [ACC] TRAIN 0.6101220244068682 / VALID 0.6113222644588522\n",
      "epoch: 14\n",
      " [LOSS] TRAIN 0.6600183706613607 / VALID 0.6552560813643021\n",
      " [ACC] TRAIN 0.609788624419366 / VALID 0.6139227845926813\n",
      "epoch: 15\n",
      " [LOSS] TRAIN 0.6773766594155038 / VALID 0.6729899748703746\n",
      " [ACC] TRAIN 0.5868507035058237 / VALID 0.5901180236106826\n",
      "epoch: 16\n",
      " [LOSS] TRAIN 0.7060573844899494 / VALID 0.7029639286478893\n",
      " [ACC] TRAIN 0.5501767020468125 / VALID 0.5633126625503915\n",
      "epoch: 17\n",
      " [LOSS] TRAIN 0.6577792006023186 / VALID 0.6542053654995602\n",
      " [ACC] TRAIN 0.6119890644954603 / VALID 0.6149229846326894\n",
      "epoch: 18\n",
      " [LOSS] TRAIN 0.6584282810031282 / VALID 0.6549871436331982\n",
      " [ACC] TRAIN 0.6138561045661675 / VALID 0.6159231846458699\n",
      "epoch: 19\n",
      " [LOSS] TRAIN 0.6595760911172522 / VALID 0.655527446276666\n",
      " [ACC] TRAIN 0.6124558245379773 / VALID 0.6155231046328475\n",
      "epoch: 20\n",
      " [LOSS] TRAIN 0.6835126368182496 / VALID 0.6787460763994801\n",
      " [ACC] TRAIN 0.5843835434111117 / VALID 0.5907181436644957\n",
      "epoch: 21\n",
      " [LOSS] TRAIN 0.6582842159964383 / VALID 0.6550839810067116\n",
      " [ACC] TRAIN 0.6127892245274668 / VALID 0.6175235047367101\n",
      "epoch: 22\n",
      " [LOSS] TRAIN 0.7023527491980571 / VALID 0.699355679050544\n",
      " [ACC] TRAIN 0.5587117423882141 / VALID 0.5743148629964412\n",
      "epoch: 23\n",
      " [LOSS] TRAIN 0.6711778157852869 / VALID 0.6691416062147862\n",
      " [ACC] TRAIN 0.5978529039339884 / VALID 0.5981196239486316\n",
      "epoch: 24\n",
      " [LOSS] TRAIN 0.6828216939256407 / VALID 0.6786813632348319\n",
      " [ACC] TRAIN 0.5832499833498715 / VALID 0.5943188637787162\n",
      "epoch: 25\n",
      " [LOSS] TRAIN 0.6570317336676271 / VALID 0.6521869896173144\n",
      " [ACC] TRAIN 0.6158565046700302 / VALID 0.6191238247887996\n",
      "epoch: 26\n",
      " [LOSS] TRAIN 0.6706077582384496 / VALID 0.6668790488105747\n",
      " [ACC] TRAIN 0.6008535040659356 / VALID 0.605321064221785\n",
      "epoch: 27\n",
      " [LOSS] TRAIN 0.6952678698010784 / VALID 0.6945609689283476\n",
      " [ACC] TRAIN 0.5829165833265995 / VALID 0.579915983202601\n",
      "epoch: 28\n",
      " [LOSS] TRAIN 0.6600561458333029 / VALID 0.6571571956422\n",
      " [ACC] TRAIN 0.6139227845588986 / VALID 0.6167233446927805\n",
      "epoch: 29\n",
      " [LOSS] TRAIN 0.6648741596625981 / VALID 0.66374644849939\n",
      " [ACC] TRAIN 0.6087884243833325 / VALID 0.6081216243606349\n",
      "epoch: 30\n",
      " [LOSS] TRAIN 0.649812184779447 / VALID 0.6466421178875744\n",
      " [ACC] TRAIN 0.6239914649835052 / VALID 0.626125225080779\n",
      "epoch: 31\n",
      " [LOSS] TRAIN 0.7226836623219941 / VALID 0.7196186380186995\n",
      " [ACC] TRAIN 0.5702473828118826 / VALID 0.5693138627964012\n",
      "epoch: 32\n",
      " [LOSS] TRAIN 0.6727145859668467 / VALID 0.6686147486216832\n",
      " [ACC] TRAIN 0.600586784047318 / VALID 0.6015203040667738\n",
      "epoch: 33\n",
      " [LOSS] TRAIN 0.6588899502000976 / VALID 0.6580246987522161\n",
      " [ACC] TRAIN 0.6143228645888124 / VALID 0.6147229445948794\n",
      "epoch: 34\n",
      " [LOSS] TRAIN 0.6673798829291259 / VALID 0.6650849436134977\n",
      " [ACC] TRAIN 0.6059211842527451 / VALID 0.6111222244806589\n",
      "epoch: 35\n",
      " [LOSS] TRAIN 0.6481040368964373 / VALID 0.6448199745654392\n",
      " [ACC] TRAIN 0.6241248249709547 / VALID 0.6225245049367502\n",
      "epoch: 36\n",
      " [LOSS] TRAIN 0.648393177017177 / VALID 0.6437717662212443\n",
      " [ACC] TRAIN 0.6263252650848061 / VALID 0.6273254651168653\n",
      "epoch: 37\n",
      " [LOSS] TRAIN 0.645573455071047 / VALID 0.6427025143612287\n",
      " [ACC] TRAIN 0.6279922651435372 / VALID 0.6295259051869979\n",
      "epoch: 38\n",
      " [LOSS] TRAIN 0.7265707665525644 / VALID 0.727015693072296\n",
      " [ACC] TRAIN 0.5485763819748508 / VALID 0.5605121024264458\n",
      "epoch: 39\n",
      " [LOSS] TRAIN 0.7208144823900897 / VALID 0.7189603237825337\n",
      " [ACC] TRAIN 0.5537774221828924 / VALID 0.5641128226002828\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "result = train(net, X, y, optimizer, criterion, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(result['final_epoch']), result[\"tloss\"], label='training loss')\n",
    "plt.plot(range(result['final_epoch']), result[\"vloss\"], label='validation loss')\n",
    "plt.axvline(x=result['best_epoch'], label='early stop', color='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(result['final_epoch']), result[\"taccs\"], label='training accuracy')\n",
    "plt.plot(range(result['final_epoch']), result[\"vaccs\"], label='validation accuracy')\n",
    "plt.axvline(x=result['best_epoch'], label='early stop', color='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(3, 32, 512)\n",
    "net.train(X, y, result['best_epoch'], optimizer, criterion, batch_size, learning_rate)\n",
    "y = []\n",
    "split = 10\n",
    "step = int(len(submission) / split)\n",
    "for i in range(0, len(submission), step):\n",
    "    batch = torch.Tensor(submission[i : i + step])\n",
    "    batch = batch.view(-1, 3, 64, 64)\n",
    "    batch = batch.cuda()\n",
    "\n",
    "    y += list(map(int, net.forward(batch).max(1)[1].cpu()))\n",
    "\n",
    "y = pd.DataFrame(y)\n",
    "y.index.name = \"id\"\n",
    "y.columns = [\"label\"]\n",
    "y = y.replace(0, \"Cat\")\n",
    "y = y.replace(1, \"Dog\")\n",
    "y.index += 1\n",
    "y.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
