{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.utils.data as utils\n",
    "from matplotlib import image as img\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, y, batch_size, lr, p=15, rep=5):\n",
    "    X = torch.Tensor(X).float()\n",
    "    N = len(X)\n",
    "    y = torch.Tensor(y).long()\n",
    "    train_size = int(0.9 * N)\n",
    "    valid_size = N - train_size\n",
    "    train_index, valid_index = torch.utils.data.random_split(\n",
    "        np.arange(N), [train_size, valid_size]\n",
    "    )\n",
    "    train_dataset = utils.TensorDataset(X[train_index], y[train_index])\n",
    "    valid_dataset = utils.TensorDataset(X[valid_index], y[valid_index])\n",
    "    dataloader = utils.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "    validloader = utils.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    epoch_list = [0]\n",
    "    best_vloss = float(\"inf\")\n",
    "#     k = 0\n",
    "    for i in range(rep):\n",
    "        print(\"Training with batch_size: {}, learning_rate: {}\".format(batch_size, lr))\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "        j = 0\n",
    "        epoch = epoch_list[-1]\n",
    "        if i >= 1:\n",
    "            net.load_state_dict(best_state)\n",
    "        while j < p:\n",
    "            epoch += 1\n",
    "            print(\"epoch: {}\".format(epoch))\n",
    "            net.train_mode()\n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                X, y = batch\n",
    "                X = X.view(-1, 3, 64, 64)\n",
    "                y = y.view(-1)\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "                loss = nn.CrossEntropyLoss()(net.forward(X), y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            net.eval_mode()\n",
    "            train_loss, train_acc = net.evaluate(dataloader)\n",
    "            valid_loss, valid_acc = net.evaluate(validloader)\n",
    "\n",
    "            train_accs.append(train_acc)\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_accs.append(valid_acc)\n",
    "            if valid_loss < best_vloss:\n",
    "                print(\"Best epoch yet\")\n",
    "                best_vacc = valid_acc\n",
    "                best_vloss = valid_loss\n",
    "                best_state = net.state_dict()\n",
    "                best_epoch = epoch\n",
    "                j = 0\n",
    "            else:\n",
    "                j += 1\n",
    "\n",
    "            print(\" [LOSS] TRAIN {} / VALID {}\".format(train_loss, valid_loss))\n",
    "            print(\" [ACC] TRAIN {} / VALID {}\".format(train_acc, valid_acc))\n",
    "        epoch_list.append(best_epoch)\n",
    "        # batch_size = int(batch_size / 2)\n",
    "#         if k % 2 == 0:\n",
    "#             lr = lr / 2.0\n",
    "#         else:\n",
    "#             lr = lr / 5.0\n",
    "#         k += 1\n",
    "    best_model = {\n",
    "        \"best_state\": best_state,\n",
    "        \"epoch_list\": epoch_list,\n",
    "        \"real_epochs\": [e + i * p for i, e in enumerate(epoch_list)],\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"vacc\": best_vacc,\n",
    "        \"best_vloss\": best_vloss,\n",
    "        \"vloss\": valid_losses,\n",
    "        \"taccs\": train_accs,\n",
    "        \"tloss\": train_losses,\n",
    "        \"vaccs\": valid_accs,\n",
    "        \"convs\": net.conv_size,\n",
    "        \"lins\": net.lin_size,\n",
    "    }\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def accuracy(y_pred, target):\n",
    "    correct = torch.eq(y_pred.max(1)[1], target).sum().type(torch.FloatTensor)\n",
    "    return correct / len(target)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = x.data\n",
    "        shape = data.shape\n",
    "        size = int(shape[0] * shape[1])\n",
    "        drop_idx = np.random.choice(\n",
    "            np.arange(size), replace=False, size=int(size * self.p)\n",
    "        )\n",
    "        data = data.flatten()\n",
    "        data[drop_idx] = 0\n",
    "        data = data.reshape(shape)\n",
    "        x.data = data\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, conv_size, lin_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.drop = True\n",
    "        self.lin_size = lin_size\n",
    "        self.conv_size = conv_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_size, conv_size, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(conv_size, conv_size, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(conv_size, conv_size * 2, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(conv_size * 2, conv_size * 2, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(conv_size * 2, conv_size * 4, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(conv_size * 4, conv_size * 4, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            Flatten(),\n",
    "            nn.Linear(conv_size * 4 * 4 * 4, lin_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(lin_size, int(lin_size/2),\n",
    "            nn.ReLU(True)),\n",
    "            nn.Linear(int(lin_size/2), 2)\n",
    "        ).cuda()\n",
    "#         self.dropout = nn.Sequential(Dropout()).cuda()\n",
    "#         self.lin1 = nn.Sequential(nn.Linear(lin_size, int(lin_size/2), nn.ReLU(True))).cuda()\n",
    "#         self.lin2 = nn.Sequential( nn.Linear(int(lin_size/2), 2)).cuda()\n",
    "\n",
    "    def eval_mode(self):\n",
    "        self.drop = False\n",
    "\n",
    "    def train_mode(self):\n",
    "        self.drop = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "#         dat = self.model(x)\n",
    "#         if self.drop:\n",
    "#             dat = self.dropout(dat)\n",
    "#         dat = self.lin1(dat)\n",
    "#         if self.drop:\n",
    "#             dat = self.dropout(dat)\n",
    "#         return self.lin2(dat)\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval_mode()\n",
    "        LOSSES = 0\n",
    "        ACCURACY = 0\n",
    "        COUNTER = 0\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X.view(-1, 3, 64, 64)\n",
    "            y = y.view(-1)\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(self.forward(X), y)\n",
    "            acc = accuracy(self.forward(X), y)\n",
    "            n = y.size(0)\n",
    "            LOSSES += loss.sum().data.cpu().numpy() * n\n",
    "            ACCURACY += acc.sum().data.cpu().numpy() * n\n",
    "            COUNTER += n\n",
    "\n",
    "        floss = LOSSES / float(COUNTER)\n",
    "        faccuracy = ACCURACY / float(COUNTER)\n",
    "        return floss, faccuracy\n",
    "\n",
    "    def train(self, X, y, epochs, batch_size, lr):\n",
    "        X = torch.Tensor(X).float()\n",
    "        y = torch.Tensor(y).long()\n",
    "        train_dataset = utils.TensorDataset(X, y)\n",
    "        dataloader = utils.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "        )\n",
    "        train_accs = []\n",
    "        train_losses = []\n",
    "        for i in range(epochs[1:]):\n",
    "            optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "            k=0\n",
    "            for e in range(epochs[i - 1], epochs[i]):\n",
    "                self.train_mode()\n",
    "                for batch in dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    X, y = batch\n",
    "                    X = X.view(-1, 3, 64, 64)\n",
    "                    y = y.view(-1)\n",
    "                    X = X.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                    loss = nn.CrossEntropyLoss()(self.forward(X), y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                self.eval_mode()\n",
    "                train_loss, train_acc = self.evaluate(dataloader)\n",
    "\n",
    "                train_accs.append(train_acc)\n",
    "                train_losses.append(train_loss)\n",
    "\n",
    "                print(\"epoch: {}\".format(e))\n",
    "                print(\"TRAIN : [LOSS] {} / [ACC] {}\".format(train_loss, train_acc))\n",
    "#             batch_size = int(batch_size / 2)\n",
    "#             if k % 2 == 0:\n",
    "#                 lr = lr / 5.0\n",
    "#             else:\n",
    "#                 lr = lr / 2.0\n",
    "#             k += 1\n",
    "        print(\"Training Done\")\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(cats_path, dogs_path, test_path):\n",
    "    train_set = []\n",
    "    target = []\n",
    "    n_cats = len(os.listdir(cats_path))\n",
    "    for i in range(1, n_cats):\n",
    "        f = '{}.Cat.jpg'.format(i)\n",
    "        mat = img.imread(cats_path + f)\n",
    "        if not len(mat.shape) == 3:\n",
    "            mat = np.array((mat, mat, mat)).T\n",
    "        train_set.append(mat)\n",
    "        target.append(0)\n",
    "\n",
    "    n_dogs = len(os.listdir(dogs_path))\n",
    "    for i in range(1, n_dogs):\n",
    "        f = '{}.Dog.jpg'.format(i)\n",
    "        mat = img.imread(dogs_path + f)\n",
    "        if not len(mat.shape) == 3:\n",
    "            mat = np.array((mat, mat, mat)).T\n",
    "        train_set.append(mat)\n",
    "        target.append(1)\n",
    "\n",
    "    train_set = np.asarray(train_set)\n",
    "    target = np.asarray(target)\n",
    "    \n",
    "    test_set = []\n",
    "    n_test = len(os.listdir(test_path))\n",
    "    for i in range(1, n_test):\n",
    "        f = '{}.jpg'.format(i)\n",
    "        mat = img.imread(test_path + f)\n",
    "        if not len(mat.shape) == 3:\n",
    "            mat = np.array((mat, mat, mat)).T\n",
    "        test_set.append(mat)\n",
    "\n",
    "    return train_set/255.0, target, np.asarray(test_set)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_path = \"/home/arthur/git/IFT6135_assignment_1/testset/test/\"\n",
    "dogs_path = \"/home/arthur/git/IFT6135_assignment_1/trainset/Dog/\"\n",
    "cats_path = \"/home/arthur/git/IFT6135_assignment_1/trainset/Cat/\"\n",
    "X, y, submission = create_dataset(cats_path, dogs_path, test_path)\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network contains 3374914 parameters.\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 64, 512)\n",
    "par = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The network contains {} parameters.\".format(par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch_size: 70, learning_rate: 0.001\n",
      "epoch: 1\n",
      "Best epoch yet\n",
      " [LOSS] TRAIN 0.6932245090448159 / VALID 0.6930623888969422\n",
      " [ACC] TRAIN 0.4992776245945267 / VALID 0.506500009894371\n",
      "epoch: 2\n",
      " [LOSS] TRAIN 0.6931966474968265 / VALID 0.6930687415599823\n",
      " [ACC] TRAIN 0.49927762343528986 / VALID 0.5065000066161156\n",
      "epoch: 3\n",
      " [LOSS] TRAIN 0.6931783003617881 / VALID 0.6930777424573898\n",
      " [ACC] TRAIN 0.49927762424675565 / VALID 0.5065000051259995\n",
      "epoch: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ac1bcc5a933a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4144f39a0343>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, X, y, batch_size, lr, p, rep)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 70\n",
    "learning_rate = 0.001\n",
    "\n",
    "result = train(net, X, y, batch_size, learning_rate, p=15, rep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(result[\"tloss\"])), result[\"tloss\"], label='training loss')\n",
    "plt.plot(range(len(result[\"tloss\"])), result[\"vloss\"], label='validation loss')\n",
    "for e in result['real_epochs']:\n",
    "    plt.axvline(x=e, label='learning rate change' if e==0 else None, color='green')\n",
    "plt.axvline(x=result['real_epochs'][-2], label='early stop', color='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(result[\"taccs\"])), result[\"taccs\"], label='training accuracy')\n",
    "plt.plot(range(len(result[\"vaccs\"])), result[\"vaccs\"], label='validation accuracy')\n",
    "for e in result['real_epochs']:\n",
    "    plt.axvline(x=e, label='learning rate change' if e==0 else None, color='green')\n",
    "plt.axvline(x=result['real_epochs'][-2], label='early stop', color='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testlab import labs\n",
    "\n",
    "labs = torch.Tensor(labs)\n",
    "batch = torch.Tensor(submission[:500])\n",
    "batch = batch.view(-1, 3, 64, 64)\n",
    "batch = batch.cuda()\n",
    "\n",
    "print(accuracy(net.forward(batch), labs.long().cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net = Net(3, 32, 512)\n",
    "test_net.train(X, y, result['epoch_list'], batch_size, learning_rate)\n",
    "split = 10\n",
    "step = int(len(submission) / split)\n",
    "for i in range(0, len(submission), step):\n",
    "    batch = torch.Tensor(submission[i : i + step])\n",
    "    batch = batch.view(-1, 3, 64, 64)\n",
    "    batch = batch.cuda()\n",
    "\n",
    "    y += list(map(int, test_net.forward(batch).max(1)[1].cpu()))\n",
    "\n",
    "y = pd.DataFrame(y)\n",
    "y.index.name = \"id\"\n",
    "y.columns = [\"label\"]\n",
    "y = y.replace(0, \"Cat\")\n",
    "y = y.replace(1, \"Dog\")\n",
    "y.index += 1\n",
    "y.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net = Net(3, 32, 512)\n",
    "test_net.load_state_dict(result['best_state'])\n",
    "split = 10\n",
    "step = int(len(submission) / split)\n",
    "y = []\n",
    "for i in range(0, len(submission), step):\n",
    "    batch = torch.Tensor(submission[i : i + step])\n",
    "    batch = batch.view(-1, 3, 64, 64)\n",
    "    batch = batch.cuda()\n",
    "\n",
    "    y += list(map(int, test_net.forward(batch).max(1)[1].cpu()))\n",
    "\n",
    "y = pd.DataFrame(y)\n",
    "y.index.name = \"id\"\n",
    "y.columns = [\"label\"]\n",
    "y = y.replace(0, \"Cat\")\n",
    "y = y.replace(1, \"Dog\")\n",
    "y.index += 1\n",
    "y.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
