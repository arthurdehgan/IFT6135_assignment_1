{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.utils.data as utils\n",
    "from matplotlib import image as img\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, y, optimizer, criterion, batch_size, lr, p=15):\n",
    "    X = torch.Tensor(X).float()\n",
    "    N = len(X)\n",
    "    y = torch.Tensor(y).long()\n",
    "    train_size = int(0.75 * N)\n",
    "    valid_size = N - train_size\n",
    "    train_index, valid_index = torch.utils.data.random_split(\n",
    "        np.arange(N), [train_size, valid_size]\n",
    "    )\n",
    "    train_dataset = utils.TensorDataset(X[train_index], y[train_index])\n",
    "    valid_dataset = utils.TensorDataset(X[valid_index], y[valid_index])\n",
    "    dataloader = utils.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "    validloader = utils.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "    j = 0\n",
    "    epoch = 0\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_vloss = float(\"inf\")\n",
    "    while j < p:\n",
    "        epoch += 1\n",
    "        net.train_mode()\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            X, y = batch\n",
    "            X = X.view(-1, 3, 64, 64)\n",
    "            y = y.view(-1)\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            loss = criterion(net.forward(X), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        net.eval_mode()\n",
    "        train_loss, train_acc = net.evaluate(dataloader, criterion)\n",
    "        valid_loss, valid_acc = net.evaluate(validloader, criterion)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        if valid_loss < best_vloss:\n",
    "            best_vacc = valid_acc\n",
    "            best_vloss = valid_loss\n",
    "            best_net = net\n",
    "            best_epoch = epoch\n",
    "            j = 0\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "        print(\" [LOSS] TRAIN {} / VALID {}\".format(train_loss, valid_loss))\n",
    "        print(\" [ACC] TRAIN {} / VALID {}\".format(train_acc, valid_acc))\n",
    "    best_model = {\n",
    "        \"net\": best_net.state_dict(),\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"final_epoch\": epoch,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"vacc\": best_vacc,\n",
    "        \"vloss\": valid_losses,\n",
    "        \"taccs\": train_accs,\n",
    "        \"tloss\": train_losses,\n",
    "        \"vaccs\": valid_accs,\n",
    "        \"convs\": net.conv_size,\n",
    "        \"lins\": net.lin_size,\n",
    "    }\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def accuracy(y_pred, target):\n",
    "    correct = torch.eq(y_pred.max(1)[1], target).sum().type(torch.FloatTensor)\n",
    "    return correct / len(target)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p=0.3):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = x.data\n",
    "        shape = data.shape\n",
    "        size = int(shape[0] * shape[1])\n",
    "        drop_idx = np.random.choice(\n",
    "            np.arange(size), replace=False, size=int(size * self.p)\n",
    "        )\n",
    "        data = data.flatten()\n",
    "        data[drop_idx] = 0\n",
    "        data = data.reshape(shape)\n",
    "        x.data = data\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, conv_size, lin_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.drop = True\n",
    "        self.lin_size = lin_size\n",
    "        self.conv_size = conv_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_size, conv_size, 3, 1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(conv_size, conv_size * 2, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(conv_size * 2, conv_size * 2, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(3, 3),\n",
    "            nn.ReLU(True),\n",
    "            Flatten(),\n",
    "            nn.Linear(conv_size * 162, lin_size)).cuda()\n",
    "        self.dropout = nn.Sequential(Dropout()).cuda()\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(lin_size, 2),\n",
    "        ).cuda()\n",
    "\n",
    "    def eval_mode(self):\n",
    "        self.drop = False\n",
    "        \n",
    "    def train_mode(self):\n",
    "        self.drop = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dat = self.model(x)\n",
    "        if self.drop:\n",
    "            dat = self.dropout(dat)\n",
    "        return self.lin(dat)\n",
    "\n",
    "    def evaluate(self, dataloader, criterion):\n",
    "        self.eval_mode()\n",
    "        LOSSES = 0\n",
    "        ACCURACY = 0\n",
    "        COUNTER = 0\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X.view(-1, 3, 64, 64)\n",
    "            y = y.view(-1)\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            loss = criterion(self.forward(X), y)\n",
    "            acc = accuracy(self.forward(X), y)\n",
    "            n = y.size(0)\n",
    "            LOSSES += loss.sum().data.cpu().numpy() * n\n",
    "            ACCURACY += acc.sum().data.cpu().numpy() * n\n",
    "            COUNTER += n\n",
    "\n",
    "        floss = LOSSES / float(COUNTER)\n",
    "        faccuracy = ACCURACY / float(COUNTER)\n",
    "        return floss, faccuracy\n",
    "    \n",
    "    def train(self, X, y, epochs, optimizer, criterion, batch_size, lr):\n",
    "        train_dataset = utils.TensorDataset(X, y)\n",
    "        dataloader = utils.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "        )\n",
    "        train_accs = []\n",
    "        train_losses = []\n",
    "        for e in epochs:\n",
    "            net.train_mode()\n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                X, y = batch\n",
    "                X = X.view(-1, 3, 64, 64)\n",
    "                y = y.view(-1)\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "                loss = criterion(net.forward(X), y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            net.eval_mode()\n",
    "            train_loss, train_acc = net.evaluate(dataloader, criterion)\n",
    "\n",
    "            train_accs.append(train_acc)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            print(\"epoch: {}\".format(epoch))\n",
    "            print(\"TRAIN : [LOSS] {} / [ACC] {}\".format(train_loss, train_acc))\n",
    "        print('Training Done')\n",
    "\n",
    "\n",
    "def create_dataset(cats_path, dogs_path, test_path):\n",
    "    train_set = []\n",
    "    target = []\n",
    "    n_cats = len(os.listdir(cats_path))\n",
    "    for i in range(1, n_cats):\n",
    "        f = '{}.Cat.jpg'.format(i)\n",
    "        mat = img.imread(cats_path + f)\n",
    "        if not len(mat.shape) == 3:\n",
    "            mat = np.array((mat, mat, mat)).T\n",
    "        train_set.append(mat)\n",
    "        target.append(0)\n",
    "\n",
    "    n_dogs = len(os.listdir(dogs_path))\n",
    "    for i in range(1, n_dogs):\n",
    "        f = '{}.Dog.jpg'.format(i)\n",
    "        mat = img.imread(dogs_path + f)\n",
    "        if not len(mat.shape) == 3:\n",
    "            mat = np.array((mat, mat, mat)).T\n",
    "        train_set.append(mat)\n",
    "        target.append(1)\n",
    "\n",
    "    train_set = np.asarray(train_set)\n",
    "    target = np.asarray(target)\n",
    "    \n",
    "    test_set = []\n",
    "    n_test = len(os.listdir(test_path))\n",
    "    for i in range(1, n_test):\n",
    "        f = '{}.jpg'.format(i)\n",
    "        mat = img.imread(test_path + f)\n",
    "        if not len(mat.shape) == 3:\n",
    "            mat = np.array((mat, mat, mat)).T\n",
    "        test_set.append(mat)\n",
    "\n",
    "    return train_set, target, np.asarray(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_path = \"/home/arthur/git/IFT6135_assignment_1/testset/test/\"\n",
    "dogs_path = \"/home/arthur/git/IFT6135_assignment_1/trainset/Dog/\"\n",
    "cats_path = \"/home/arthur/git/IFT6135_assignment_1/trainset/Cat/\"\n",
    "X, y, submission = create_dataset(cats_path, dogs_path, test_path)\n",
    "X = X / 255.0\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network contains 2712066 parameters.\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 32, 512)\n",
    "par = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The network contains {} parameters.\".format(par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      " [LOSS] TRAIN 0.5335018332778608 / VALID 0.547415704906261\n",
      " [ACC] TRAIN 0.7401480296098957 / VALID 0.7277455491336686\n",
      "epoch: 2\n",
      " [LOSS] TRAIN 0.5456614338088578 / VALID 0.569288460862329\n",
      " [ACC] TRAIN 0.7318797092950605 / VALID 0.7113422684656141\n",
      "epoch: 3\n",
      " [LOSS] TRAIN 0.5529342847458003 / VALID 0.5733003495192142\n",
      " [ACC] TRAIN 0.7290791492029104 / VALID 0.713542708565555\n",
      "epoch: 4\n",
      " [LOSS] TRAIN 0.5160418697148154 / VALID 0.5443506056689148\n",
      " [ACC] TRAIN 0.7503500700418239 / VALID 0.7283456691576734\n",
      "epoch: 5\n",
      " [LOSS] TRAIN 0.5114465090519763 / VALID 0.5404729131317826\n",
      " [ACC] TRAIN 0.7541508301779565 / VALID 0.7343468693798364\n",
      "epoch: 6\n",
      " [LOSS] TRAIN 0.5115420582565203 / VALID 0.5440164247759105\n",
      " [ACC] TRAIN 0.7530839501432302 / VALID 0.7321464292858572\n",
      "epoch: 7\n",
      " [LOSS] TRAIN 0.507004536815013 / VALID 0.5451510280495907\n",
      " [ACC] TRAIN 0.7556844702551999 / VALID 0.7321464293097038\n",
      "epoch: 8\n",
      " [LOSS] TRAIN 0.5014519632645604 / VALID 0.5439363739256335\n",
      " [ACC] TRAIN 0.7576181903365231 / VALID 0.727545509125667\n",
      "epoch: 9\n",
      " [LOSS] TRAIN 0.5025703644302596 / VALID 0.5440031153770847\n",
      " [ACC] TRAIN 0.7604854304472383 / VALID 0.7319463892897788\n",
      "epoch: 10\n",
      " [LOSS] TRAIN 0.5058999156513921 / VALID 0.5564286947536525\n",
      " [ACC] TRAIN 0.7580849503432702 / VALID 0.7239447889935615\n",
      "epoch: 11\n",
      " [LOSS] TRAIN 0.5181500483069522 / VALID 0.5768341311969193\n",
      " [ACC] TRAIN 0.7533506701578734 / VALID 0.7143428686094847\n",
      "epoch: 12\n",
      " [LOSS] TRAIN 0.5517901858007971 / VALID 0.604617188777607\n",
      " [ACC] TRAIN 0.7280789491430302 / VALID 0.7005401080335276\n",
      "epoch: 13\n",
      " [LOSS] TRAIN 0.5095846506884283 / VALID 0.5626654179483969\n",
      " [ACC] TRAIN 0.7560845502672287 / VALID 0.7241448290015631\n",
      "epoch: 14\n",
      " [LOSS] TRAIN 0.4991807574418065 / VALID 0.559842079520154\n",
      " [ACC] TRAIN 0.7614856304753229 / VALID 0.720344068825686\n",
      "epoch: 15\n",
      " [LOSS] TRAIN 0.5338739989447245 / VALID 0.5969603077653266\n",
      " [ACC] TRAIN 0.745682469847171 / VALID 0.7119423885015421\n",
      "epoch: 16\n",
      " [LOSS] TRAIN 0.4895667908922501 / VALID 0.5624253113202081\n",
      " [ACC] TRAIN 0.7695539107940798 / VALID 0.7205441088336877\n",
      "epoch: 17\n",
      " [LOSS] TRAIN 0.5304389648845437 / VALID 0.5998051212737264\n",
      " [ACC] TRAIN 0.7486830699830928 / VALID 0.7173434686937388\n",
      "epoch: 18\n",
      " [LOSS] TRAIN 0.5228422927627517 / VALID 0.5920045981260031\n",
      " [ACC] TRAIN 0.7494165500005087 / VALID 0.7073414683055821\n",
      "epoch: 19\n",
      " [LOSS] TRAIN 0.4892423575346713 / VALID 0.5730591871710294\n",
      " [ACC] TRAIN 0.7696205908007214 / VALID 0.7143428686094847\n",
      "epoch: 20\n",
      " [LOSS] TRAIN 0.5628441302464104 / VALID 0.6516175509262238\n",
      " [ACC] TRAIN 0.7208108288523053 / VALID 0.684536907387438\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1e5d6d0e2af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnew_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "learning_rates = [0.01, 0.005, 0.0001, 0.0005]\n",
    "\n",
    "new_net = net\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for learning_rate in learning_rates:\n",
    "    optimizer = optim.SGD(new_net.parameters(), lr=learning_rate)\n",
    "    result = train(new_net, X, y, optimizer, criterion, batch_size, learning_rate)\n",
    "    new_net.load_state(resul)\n",
    "    batch_zsize /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(result['final_epoch']), result[\"tloss\"], label='training loss')\n",
    "plt.plot(range(result['final_epoch']), result[\"vloss\"], label='validation loss')\n",
    "plt.axvline(x=result['best_epoch'], label='early stop', color='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(result['final_epoch']), result[\"taccs\"], label='training accuracy')\n",
    "plt.plot(range(result['final_epoch']), result[\"vaccs\"], label='validation accuracy')\n",
    "plt.axvline(x=result['best_epoch'], label='early stop', color='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(3, 32, 512)\n",
    "net.train(X, y, result['best_epoch'], optimizer, criterion, batch_size, learning_rate)\n",
    "y = []\n",
    "split = 10\n",
    "step = int(len(submission) / split)\n",
    "for i in range(0, len(submission), step):\n",
    "    batch = torch.Tensor(submission[i : i + step])\n",
    "    batch = batch.view(-1, 3, 64, 64)\n",
    "    batch = batch.cuda()\n",
    "\n",
    "    y += list(map(int, net.forward(batch).max(1)[1].cpu()))\n",
    "\n",
    "y = pd.DataFrame(y)\n",
    "y.index.name = \"id\"\n",
    "y.columns = [\"label\"]\n",
    "y = y.replace(0, \"Cat\")\n",
    "y = y.replace(1, \"Dog\")\n",
    "y.index += 1\n",
    "y.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
